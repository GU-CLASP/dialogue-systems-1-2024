\documentclass[11pt]{article}
\usepackage[tmargin=3cm,lmargin=3cm,rmargin=3cm,bmargin=3cm]{geometry}
\usepackage{graphicx} % Required for inserting images
\usepackage[authordate]{biblatex-chicago}
\addbibresource{bib.bib}
\usepackage{listings}

\lstnewenvironment{listing}{%
    \lstset{
      language={},
      basicstyle=\ttfamily,
      extendedchars=true,
        breaklines=true,
        breakatwhitespace=true,
        breakindent=0pt,
        % border:
        frame=single,
        framerule=0.25pt,
        framesep=5pt,
        xleftmargin=6pt,
        xrightmargin=6pt,
        caption={Description of formal language for conveying meaning of user utterances in the domain at hand.},captionpos=b,label={listing:L}
    }
}{}

\title{LLM-Based NLU for Explanatory Dialogue}
\author{alexander.berman@gu.se}

\begin{document}

\maketitle

\section{Introduction}
This report presents and discusses the integration of a large language model (LLM) based natural-language understanding (NLU) component into an explanatory dialogue system. The dialogue system is a chatbot that assists users in a game that revolves around assessing whether people are introverted or extraverted based on their music preferences. This chatbot is embedded in an interactive web page together with other elements of the game such as visualizations of music preferences, audio controls and navigation buttons. (The game will be further elaborated in the course project report.)

\section{Motivation}
One of the reasons for opting for an LLM-based NLU component is to be able to interpret user utterances in a more accurate and robust way than with a simple keyword- or rule-based component. Furthermore, it was hypothesized that an LLM-based solution would require less effort in terms of development time than a more conventional machine-learning based approach (such as logistic regression or support vector machine for intent classification) due to the ability to use zero- or few-shot learning, which in principle can drastically reduce the amount of required training data. Finally, it was deemed that the fine-grained semantics underpinning the dialogue modeling in the project would not be supported well by a more conventional approach based on intent classification and entity extraction. In contrast, with an LLM the problem can be approached as a sequence-to-sequence task, i.e. as a transformation from natural language to an expression in formal language whose syntax and semantics is fed to the LLM in the prompt.

\section{Problem formulation}
In conventional approaches, NLU for dialogue systems is typically divided into two ``tasks'': intent classification and entity extraction. For example, the user utterance ``I need a ticket to Paris'' can be understood as expressing the intent \texttt{BookTicket} and the entity \texttt{Paris} (possibly assigned to the role \texttt{Destination}). In contrast, here the task as framed as a matter of expressing the meaning of a given natural-language utterance in a formal language. For example, the meaning of the utterance above might be expressed as a sequence of dialogue moves such as [\texttt{request(BookTicket)}, \texttt{answer(dest\_city(Paris))}].

In general formal terms, given a user utterance $U$ and a description $L$ of a formal language, the NLU component should return an expression $E$ in $L$ that conveys the meaning of $U$. The value of $L$ in the context of the specific domain is provided in Listing \ref{listing:L} (for source code, see \texttt{project/dialog/music\_personality/nlu\_openai\_config.yml}).

\include{L}

\section{Prompt design}
The prompt contains a description of the task and of the formal language as well as examples of inputs and outputs, and has been iterated in tandem with testing (see section \ref{sec:evaluation}). The most recent version of the prompt can be found in \texttt{project/dialog/music\_personality/nlu\_openai\_config.yml}.

\section{Implementation}
OpenAI's chat completion API is used as follows: Given the user utterance $U$, a prompt describing the task is provided as an initial system message, with $L$ embedded into the prompt. $U$ is then fed as a user message. As completion, an assistant message $M$ is obtained as output (see \texttt{project/dialog/music\_personality/nlu\_openai.py}). 

In order for the output $M$ to be used as input to the dialogue manager, it also needs to be parsed from a string (semantic expression) into a dialogue move. The dialogue system at hand is implemented in Python and uses custom types and classes to represent dialogue moves and their content. For example, instances of the class \texttt{Ask} denote ask-moves, and contain an instance of the class \texttt{Question}. To streamline the process of parsing semantic expressions, $L$ is formulated in way that literally reflects the syntax of the internal semantics of the dialogue system. Hence, output from the LLM can be treated as executable Python code and can in principle be parsed by passing it to the built-in Python function \texttt{eval}. However, such a solution would not be secure since the output from LLM cannot be guaranteed to abide to the instructed form and content. Instead, a secure parser has been implemented. This parser uses Python's \texttt{ast} module, which enables Python code to be parsed into an abstract syntax tree. The parser iterates this tree recursively to construct a semantic structure and raises an exception for unsupported (and potentially unsafe) expressions (see \texttt{project/dialog/semantic\_serialization.py}).

\section{Evaluation}
\label{sec:evaluation}
At this point, the integration has only been evaluated unsystematically by manually testing the dialogue system and purposely trying user utterances other than the ones exemplified in the prompt. Initially, it was noted that the LLM (GPT 3.5) generated output not only in the formal language, but also accompanying elaborations in natural language. This problem was solved by adding an instruction to the prompt to ``(o)nly output the semantic representation without any surrounding content.'' However, the model would occassionally generate syntactically invalid output (mismatched parentheses). After switching to GPT 4, no occurrences of invalid syntax have been encountered.

As for accuracy, the impression so far is that GPT 4 performs very well. One interesting observation concerns deixis: when the utterance ``"why does that make it more likely that the person is extraverted?'' was tried, in response to the system asserting that the person likes silent music, GPT produced the following interpretation: \texttt{Ask(Why(Supports(Antecedent, Extraverted())))}. Worth noting is that the first argument to \texttt{Supports}, semantically corresponding to the deictic demonstrative ``that'', is \texttt{Antecedent}, i.e. underspecified. In other words, GPT seems to capture the deictic function of ``that'' in a relevant way, without having seen any examples of this kind. (At this point, however, the dialogue manager does not support underspecified semantic content. Alternatively, one can consider letting the LLM perform deictic resolution itself by feeding dialogue history as input.)

In future work it would be useful to evaluate performance more systematically, e.g. by collecting dialogue data, including user utterances and GPT's interpretations and annotating expected (correct) interpretations. This way, accuracy can be measured and potential misinterpretations be studied.

\section{Conclusions}


\end{document}
